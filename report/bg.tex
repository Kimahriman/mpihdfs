\section{Existing Methods}
\label{sec:bg}
The Hadoop community makes a lot of effort~\cite{fuse} to ease the use of HDFS to
facilitate regular file maintenance. For example, there are many tools to support
operations using standard Unix
utilities such as `ls', `cd', `cp', `mkdir', `find', `grep', or use standard
POSIX libraries like open, write, read, close from C, C++, etc. Such efforts
usually use the HDFS library, RPC, protobuf or NFS proxy, and are exposed to users
through FUSE or NFS.

Projects~\cite{fuse, hdfs-fuse, fuse-j} are built on the File System In Userspace (FUSE) package. These projects enable
HDFS to be mounted as a standard file system using the mount command. On the
back end, these projects interact with HDFS using the HDFS library, either native
library or Java library. The problem with the Hadoop Library is that it doesn't
support random writes, thus all writes need to be sequential. This limitation
prevents MPI parallel write operations to be implemented through the library.
Here we choose
to use the contrib/fuse-dfs~\cite{fuse} as our target for evaluation since it's
provided by the official Hadoop team and shipped with Hadoop releases. However,
because of the limitation of the library, we can only evaluate parallel reads and
sequential writes.

Unlike
these projects, projects such as native-hdfs-fuse~\cite{native} do not use the
HDFS library or otherwise start a JVM - it constructs and sends the protocol buffer
messages itself. This project also uses FUSE to interact with users. Another
project Hadoofus~\cite{hadoofus} provides only a C
API for directly calling Namenode RPCs and performing Datanode block read and
write operations, as well as a Hadoop Library compatible interface. Such methods
avoid using the HDFS library by directly interacting with a Namenode using some
protocol. These methods support random file writes. However, the projects are
not well maintained and they corrupt when we try to install them. Hence, we have
no way to evaluate such protocol communication based methods.

HDFS NFS Proxy~\cite{proxy} and NFS Gateway~\cite{nfs} are two representative
NFS solutions. Both export HDFS as an NFS. The former creates an NFS4 mount point
for HDFS for clients to use. However, the latest version of MPICH that
we use only supports NFS3. Thus the solution is not suitable for MPI-IO. On the
other hand, NFS Gateway is a Hadoop official effort that supports NFS3. However,
it is claimed that file append is supported but random write is not supported.
NFS Gateway requires to stop system
critical service such as nfs and rpcbind. Since our test bed is in a public
cluster, we could not evaluate this method here. However, we believe this method would
not perform better than NFS, and it doesn't support random write either.

The challenges we have faced through are experience can be summed up as follows.
\begin{itemize}
\item Existing methods such as FuseDFS and HDFS-NFS-Proxy are designed to provide ease of
	maintenance instead of parallel file accessing, thus only sequential
	write operations are supported. Often failure is the case when we try to
	perform parallel writes (Section~\ref{sec:exp}). 
\item MPI-IO accesses underlying file systems using standard POSIX IO system
	calls, while HDFS is not designed to be a mountable POSIX file system
	and must be accessed through its own API. The native library provided by
	HDFS gives a C/C++ compatible library that can be adopted for MPI
	applications.
\item During our development, we found that HDFS does not support 
	parallel or positional writes. The HDFS native library provides many
	checks to ensure this cannot happen. While emulating this behavior
	using backend logic should be possible, we consider this a whole topic of
	research on its own.
\item HDFS is an append-only file system, while sometimes MPI jobs want to
	modify the data in the file. To achieve that, one must rewrite the
	entire file and replace the old file. This can cause a huge overhead on
	small file updates. However, this problem is not addressed in this
	report because our study and benchmark focus on sequential write and
	parallel read throughput without consideration of file updates.
\end{itemize}

