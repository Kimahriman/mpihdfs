\section{Introduction}
MapReduce~\cite{mr} and its most popular implementation Hadoop~\cite{hadoop}
have become the dominant distributed processing framework for big data
analytics. Despite of the ease-of-use and scalibility of Hadoop, researchers
also found the limitations of Hadoop lie in for example, inter-process
communication. For such limitations, the well established Message Passing
Interface (MPI)~\cite{mpi} is more suitable due to its ability to support any
communication pattern. X. Lu et al~\cite{xlu} find that  the
message latency of MPI is about 100 times less than Hadoop
primitives. The average peak bandwidth of MPI is about
100 times higher than Hadoop RPC -- the fundamental communication
mechanism in Hadoop. Moreover, there exists data analytics workflows such as
Metagenomics~\cite{meta} that consist both compute- and communication-intensive
computations. To better conduct such workflows and to avoid data
movement between clusters~\cite{???}, resource coordination platforms such as
mesos~\cite{mesos}, omega~\cite{omega}, YARN~\cite{yarn} enable different
programming paradigms including MPI and MapReduce to co-exist in the same
cluster. Though not realized yet, hosting MPI and Hadoop in the same cluster is
highly promising as YARN claims to embrace MPI as a first class citizen.

One of the biggest challenges of co-hosting MPI and Hadoop is to decide the
underlying shared file system. As a big feature of MPI-2 standard~\cite{mpi},
MPI-IO provides parallel IO support to MPI programs and enables MPI to process
data-intensive workloads as well. Currently, such support requires an underlying
network/parallel file system such as NFS~\cite{nfs1}, PVFS~\cite{pvfs},
Lustre~\cite{lustre}, GPFS~\cite{gpfs} to achieve the best performance. However,
these file systems are focused on optimization for MPI-IO~\cite{mpipvfs,
mpilustre1, mpigpfs} and have a big network overhead on hosting Hadoop with the
absence of data locality~\cite{hadooplustre}. IBM's GPFS is originally designed
as a SAN file system as the data is striped and placed in a round-robin
fashion~\cite{gpfs}, which prevents it from being used in Hadoop. With a support
of File Placement Optimization (FPO), GPFS-FPO makes it possible to efficiently
support Hadoop.  However, GPFS is shipped with IBM SP system and is not
available as opensource as Lustre. Moreover, IBM tailors MPI-IO according to
GPFS in their own MPI implementation~\cite{mpigpfs}, which is not supported in
more wildly used implementations such as MPICH~\cite{} and
OpenMPI~\cite{openmpi}.

Another solution is to support MPI-IO on top of the distributed file systems
used by MapReduce such as GFS~\cite{} and HDFS~\cite{hdfs}. HDFS is integrated
inherently in Hadoop releases and is the default file system used in Hadoop
community. By bringing MPI to HDFS, it is possible to keep existing applications
in Hadoop ecosystem without any changes. C. Cranor et al~\cite{CMU-PDL-12-115} explore the
performance of MPI-IO on HDFS using PLFS. However, HDFS is supported as a
component of PLFS and no data locality is achieved for MPI jobs. As far as we
know, there is no such work on supporting MPI on top of HDFS directly. This
project focuses on exploring the feasibility and performance of enabling MPI-IO
on top of HDFS using existing technologies. This study is based on the
observation that MPI-IO provides great flexibility that it is possible for users
to decide the process-to-block mapping. In this work, we explore and evaluate
the existing methods including FuseDFS~\cite{fuse}, Native Library~\cite{lib},
HDFS-NFS-Proxy~\cite{proxy}. We also develop our own MPI-IO hook using the
native library provided by HDFS. In order to evaluate the performance, we also
developed a MPI-IO benchmark. This report reflects the difficulties we
encountered, whether each method can be adopted by MPI-IO, and how they performs. 

During our study and development, we encounter several
challenges including ones that are still not solved yet.
\begin{itemize}
\item Existing methods such as FuseDFS and HDFS-NFS-Proxy are originally designed to ease the file
	maintenance instead of parallel file accessing, thus only sequential
	write operations are supported. Often failure is the case when we try to
	perform parallel writes (Section~\ref{sec:}). 
\item MPI-IO accesses underlying file systems using standard POSIX I/O system
	calls, while HDFS is not designed to be a mountable POSIX file system
	and must be accessed through its own API. The native library provided by
	HDFS gives a C/C++ compatible library that can be adopted for MPI files.
	However, MPI-IO does not have the corresponding support to access
	through the library. We develop a hook which enables MPI-IO to access
	HDFS files using the native library transparently so that the only thing
	the user needs to provide is the HFDS URL to locate the file
	(Section~\ref{sec:}).
\item During our development, we found that HDFS does not support 
{\color{red} what writes? parallel writes? please describe here. }
\item HDFS is an append-only file system, while sometimes MPI jobs want to
	modify the data in the file. To achieve that, one must rewrite the
	entire file and replace the old file. This can cause a huge overhead on
	small file updates. However, this problem is not addressed in this
	report because our study and benchmark focus on parallel reads and
	writes throughput without consideration of file updates.
\end{itemize}

