\section{Experiments}
\label{sec:exp}

\begin{figure}[t]
\begin{minipage}{3in}
\begin{center}
\epsfig{figure=read1.eps, width=3in}
\caption{\small Read bandwidth of Local file access, NFS, {\proj} and FuseDFS
with the increase of process numbers.}
\label{fig:read1}
\vspace{-6pt}
\end{center}
\end{minipage}
\hspace{0.02in}
%\begin{figure}[t]
\begin{minipage}{3in}
\begin{center}
\epsfig{figure=read2.eps, width=3in}
\caption{\small Read bandwidth of NFS, {\proj} and FuseDFS
with the increase of process numbers.}
%The x-axis is the number of containers requested.}
\label{fig:read2}
\vspace{-6pt}
\end{center}
\end{minipage}
\end{figure}

\begin{figure}[t]
\begin{minipage}{3in}
\begin{center}
\epsfig{figure=read3.eps, width=3in}
\caption{\small Read bandwidth of NFS, {\proj} and FuseDFS on HDFS with 3
replicas.}
\label{fig:read3}
\vspace{-6pt}
\end{center}
\end{minipage}
\hspace{0.02in}
%\begin{figure}[t]
\begin{minipage}{3in}
\begin{center}
\epsfig{figure=read4.eps, width=3in}
\caption{\small Read bandwidth of {\proj} and FuseDFS
with 1 replica and 3 replicas respectively.}
%The x-axis is the number of containers requested.}
\label{fig:read4}
\vspace{-6pt}
\end{center}
\end{minipage}
\end{figure}

In this section we compare the performance of the most adopted method by far,
namely FuseDFS, with {\proj}. More specifically, by using our MPI-IO benchmark,
we measure the performance by the bandwidth of parallel reads and sequential
writes. Here we also display the errors we encountered when we perform parallel
writes on FuseDFS. Our experiments are set up in a three-node HDFS cluster,
where each node contains two Intel Quad-core Xeon E5462 $2.8~GHz$ processors,
$12~MB$ L2 cache, $8~GB$ memory and one $320~GB$ Seagate ST3320820AS\_P SATA
disk. Nodes are connected using $1~Gbps$ Ethernet. An NFS file system is mounted
in each of the nodes. We use the latest MPICH
version $3.1$ as our MPI implementation. Our benchmarks perform parallel reads
and sequential writes of a single file. For reads, each process gets a proportional
amount of the file to read, and the times measured are from when the first process
starts reading to when the last process finishes reading. All of the processes run
simultaneously, so MPI routines are used to determine the earliest starting time
and the latest finishing time. For writing, just a single process is used to
write a sequential file.

\subsection{Read Performance}
Figure~\ref{fig:read1} shows the read bandwidths for local file access, NFS, {\proj},
and FuseDFS. Here we configure HDFS to have one replica for each file block with
the default block size (64MB). To match with the default block size of HDFS, we
also configure the benchmark with 64MB access block size so that each process
access 64MB of data each time. Note that we only use two of the nodes to set up
HDFS in this case. As we expected, the local file access achieves
the best throughput compared with the other three methods. However, these other three
methods show mostly the same performance. The
difference between the local file access and the other methods are network
connection. Both HDFS and NFS methods fetch data over the network.

To better compare the performance difference between HDFS methods and NFS. We
take away the ``Local'' in the figure as shown in Figure~\ref{fig:read2}. We 
can see that {\proj} performs better than FuseDFS method, which is expected
because {\proj} does not entail the overhead of FUSE. The more interesting thing
here is the comparison between NFS and {\proj}. When there are small
number of processes, {\proj} performs even better than NFS. However, after the
number of processes reaches 6, NFS starts to perform better than {\proj}. We
suspect the reason behind is because we have only one replica in each node, and
the processors are located within the data node. With the same access block
size, some how some processes achieved data locality, thus avoiding some of the
network transfers. However, when the number of processes increases, the
underlying number of nodes is not big enough to serve the locality needs. That
causes the data to transfer through the network.

\begin{figure}[t]
\begin{center}
\epsfig{figure=write.eps}
\caption{\small Write bandwidth of Local, NFS, {\proj} and FuseDFS
with 1 replica and 3 replicas respectively.}
\label{fig:write}
\vspace{-6pt}
\end{center}
\end{figure}

To verify what we expected, we conduct another experiment where we add a data
node in the HDFS cluster and set the
number of replicas to 3, and run the same benchmark. Figure~\ref{fig:read3}
shows the results. We can see that this time not only {\proj} performs better
than NFS, FuseDFS also performs better. This is because the number of replicas
reaches 3 so that more processes can benefit from locality by reading data
locally. Until we reach 9 processors, HDFS
performs better than NFS. This is because we add one more data node to
serve more processes.


Figure~\ref{fig:read4} shows the performance of {\proj} and FuseDFS on HDFS
configured with 3 replicas and 1 replica each. The results show that {\proj}
with 3 replicas performs the best and FuseDFS with 1 replica performs worst,
which are expected. {\proj} with 1 replica and FuseDFS with 3 replicas are not
deterministic. Sometimes the overhead of FUSE is bigger than the overhead of the
network. Sometimes the opposite case is observed. 

\subsection{Write Performance}
As described earlier, Hadoop library has a limitation where it doesn't support
random writes, thus we encounter errors on both FuseDFS and {\proj}.
Table~\ref{tab:write} shows an example of error messages that we encountered
when trying to perform parallel writes on FuseDFS. According to the
wiki~\cite{fuse}, the way to support random writes is to use methods like RPC
and protobuf instead of HDFS library. However, there is no way to verify that
because there is no existing projects that work yet. By the time we realized this
problem is caused by the HDFS library, it was too late for us to change the
design of {\proj} and use protobuf instead. Hence, here we only evaluate
performance of sequential writes within one process. We run the same benchmark
that writes 1GB file with the block size of 64MB to measure the write bandwidth.
Similarly, we measure the performance on HDFS configured with 1 replica and 3
replicas respectively.
\begin{table}[t]
	\centering \small \begin{tabular}{ccc} \toprule {\bf Function} &{\bf
Mode} &{\bf Error} \\\otoprule {\tt MPI\_File\_write\_at} & {\tt CREATE|RDWR} &
	cannot open an hdfs file in O\_RDWR mode \\ {\tt MPI\_File\_write\_at} &
	{\tt CREATE|WRONLY} & cannot open an hdfs file in O\_RDWR mode \\ {\tt
MPI\_File\_write\_at} & {\tt WRONLY} & cannot open an hdfs file in O\_RDWR mode
	\\ {\tt MPI\_File\_write\_shared} & {\tt WRONLY} & cannot open an hdfs
	file in O\_RDWR mode \\ {\tt MPI\_File\_write\_shared} & {\tt APPEND}
&file open.  code: 201388309\\\bottomrule \end{tabular} \caption{\small Error
codes for parallel writes on FuseDFS.} \label{tab:write}
\end{table}

Figure~\ref{fig:write} shows the results. The results are quite surprising.
FuseDFS performs better than {\proj} and NFS both in HDFS with 1 replica and 3
replicas. {\color{red} What to say here? just say we have no idea why this
happens? I don't really know how the library triggers write, maybe you can think
of some reasons for this?} 

\subsection{Performance Overhead}
In this section we evaluate the performance overhead of {\proj} against standard
MPI-IO library when accessing normal files. Since {\proj} is designed as a hook
that determines whether we need to trigger MPI standard functions or our own.
{\color{red} find a better way to say this..} If {\proj} determines that the
operation is not an HDFS file operation, it directly hands over the control to
MPI standard functions. Therefore, we expect negligible
overhead from {\proj} when performing normal file operations. To verify this, we
conduct experiments that measure both read and write bandwidths on local files. We use MPI-IO
standard operation bandwidths as our baseline. 

Figure~\ref{fig:over} shows the results. We can see that there is almost no
performance overhead caused by {\proj}. Same results are also observed in write
operations. Specifically, {\proj} out perform native MPI-IO with 0.5\%.

\begin{figure}[t]
\begin{center}
\epsfig{figure=over1.eps}
\caption{\small Read overhead of {\proj} normalized to native MPI-IO.}
\label{fig:over}
\vspace{-6pt}
\end{center}
\end{figure}
