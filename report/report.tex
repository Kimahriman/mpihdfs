\documentclass[11pt,titlepage]{article}
% define the title
\author{Luna Xu (xuluna@cs.vt.edu) \and Adam Binford (adamq@vt.edu)}

\title{CS 5204 Project Final Report \\ A Feasible Study of MPI-IO on Top of HDFS}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
%\usepackage[pdftex,bookmarks,colorlinks]{hyperref}
\usepackage{cite}
\usepackage{color}
\usepackage{pgfgantt}
\usepackage{float}
\usepackage{url}
\usepackage{booktabs}

%fullpage â€“ Set all page margins to 1.5cm
\usepackage{fullpage}

\newcommand{\otoprule}{\midrule[\heavyrulewidth]}
%\usepackage[pdftex]{hyperref}
\begin{document}
% generates the title
\maketitle

\section{Team member}
Luna Xu (xuluna)\\
Adam Binford (adamq)

\section{Introduction}
MapReduce~\cite{mr} and its most popular implementation Hadoop~\cite{hadoop}
have become the dominant distributed processing framework for big data
analytics. Despite of the ease-of-use and scalibility of Hadoop, researchers
also found the limitations of Hadoop lie in for example, inter-process
communication. For such limitations, the well established Message Passing
Interface (MPI)~\cite{mpi} is more suitable due to its ability to support any
communication pattern. X.Lu et al~\cite{xlu} find that  the
message latency of MPI is about 100 times less than Hadoop
primitives. The average peak bandwidth of MPI is about
100 times higher than Hadoop RPC -- the fundamental communication
mechanism in Hadoop. Moreover, there exists data analytics workflows such as
Metagenomics~\cite{meta} that consist both compute- and communication-intensive
computations. To better conduct such workflow computation and to avoid data
movement between clusters~\cite{???}, resource coordination platforms such as
mesos~\cite{mesos}, omega~\cite{omega}, YARN~\cite{yarn} enable different
programming paradigms including MPI and MapReduce to co-exist in the same
cluster. Though not realized yet, hosting MPI and Hadoop in the same cluster is
highly promising as YARN claims to embrace MPI as a first class citizen.

One of the biggest challenges of this marriage is the underlying shared file
system. As a big feature of MPI-2 standard~\cite{mpi}, MPI-IO provides parallel IO support
to MPI programs and enables MPI to process data-intensive workloads as well. Currently, such
support requires an underlying network/parallel file system such as NFS~\cite{nfs1},
PVFS~\cite{pvfs}, Lustre~\cite{lustre}, GPFS~\cite{gpfs} to achieve the best
performance. However, these file systems are focused on optimization for
MPI-IO~\cite{mpipvfs, mpilustre1, mpigpfs} and have a big network overhead on hosting
Hadoop with the absence of data locality~\cite{hadooplustre}. IBM's
GPFS is originally designed as a SAN file system as the data is striped and
placed in a round-robin fashion~\cite{gpfs}, which prevents it from being
used in Hadoop. With a support of File Placement Optimization (FPO), GPFS-FPO
makes it possible to efficiently support Hadoop.
However, GPFS is shipped with
IBM SP system and is not available as opensource as Lustre. Moreover, IBM
tailors MPI-IO according to GPFS in their own MPI implementation~\cite{mpigpfs},
which is not supported in more wildly used implementations such as MPICH~\cite{}
and OpenMPI~\cite{openmpi}.

Another solution is to support MPI-IO on top of the distributed file systems
used by MapReduce such as GFS~\cite{} and HDFS~\cite{hdfs}.

users with function calls reminiscent of MPI message passing calls. MPI-IO also
lets users write and read files in a normal, i.e., blocking mode, and then in the
non-blocking mode - asynchronously - so that computations can be carried out,
while the file is being read or written in the background. Similarly,
traditional collective operations are supported so that processes can access
MPI files each on its own, or all together at the same time. 
Different from Hadoop, MPI-IO gives users flexibility so that it is possible for
users to decide the striping strategy of the file based on their needs. 
dominant programming model in scientific computing, MPI-IO enables MPI to not
only perform CPU-intensive jobs but also data-intensive jobs. 

{\color{blue}Challenges of enabling MPI-IO on top of HDFS.}
\section{Project Progress}
We have finished investigating the possible ways to mount HDFS as a regular
file system that can be interacted with by any file I/O. 
We got the throughput for manual copy, native NFS as the ideal performance,
fuse-dfs throughput for parallel read. However we could not perform parallel
write using fuse-dfs. Table~\ref{tab:write} shows the errors we encountered during our
tries. We tried using {\tt MPI\_File\_write\_at} where each process holds an
individual file pointer, as well as {\tt MPI\_File\_write\_shared} where all
processes hold a shared file pointer. We open the file using different mode and
with the combinations we get mainly two errors. The error we get from the {\tt
APPEND} mode is reported in the MPI program side, others are shown in the
fuse-dfs side. Another method that we explored is Native HDFS Fuse~\cite{native},
which utilizes only protobuf to communicate with Namenode directly. Hence no
fuse or native lib is involved. However, the program dumped a segmentation fault
when we tried to run. HDFS-NFS solution is also not successful nor
desirable because either it has requirements for specific (2.3.0) Hadoop
version~\cite{nfs} or it only supports the cloudera distribution of
Hadoop~\cite{proxy}.


We are now focusing on creating a library to 
hook MPI~\cite{mpich} I/O function calls to use the HDFS native library to interact with
HDFS. Our goal is to allow 
unmodified MPI applications to interact with HDFS by simply loading our library
at runtime. 
So far we have successfully hooked MPI functions at runtime, and verified our
functions were being 
called. Additionally, we have read from and written to files in our running HDFS
using the HDFS native 
library. When reading a single file from multiple processes, we have observed an
increase in bandwidth 
when increasing processes. This confirms that multiple processes can read from the
same file at once using 
the native library. To complement this work, we have developed scripts to
compile and run these HDFS 
native library applications easily.

\section{Future work}
The final steps we have to do are implementing the necessary MPI I/O functions
in our hooking library to 
use the HDFS native library as the file I/O method. We have already done each of
these pieces 
individually, hooking and using the native library, we simply must combine them.
The hooking functions 
need to be able to use the parameters they are given to seamlessly work with
HDFS without the MPI 
program knowing anything is different. 

Additionally, we must find out if it is possible to implement some support for
writing to HDFS through 
the hooked MPI routines. The native library only allows appending to a file, and
only one thread can 
access a file for writing at one time. We must either modify the behavior of the
I/O of the MPI program 
or set restrictions on what MPI programs running on HDFS are allowed to do.
Finally, we must simplify 
the scripts required for our solution to work to put as small of a burden on the
user as possible.

\begin{table}[t]
	\centering
	\small
	\begin{tabular}{ccc}
		\toprule
	{\bf Function} &{\bf Mode} &{\bf Error} \\\otoprule
		{\tt MPI\_File\_write\_at} & {\tt CREATE|RDWR} & cannot open an
		hdfs file in O\_RDWR mode \\
		{\tt MPI\_File\_write\_at} & {\tt CREATE|WRONLY} & cannot open an
		hdfs file in O\_RDWR mode \\
		{\tt MPI\_File\_write\_at} & {\tt WRONLY} & cannot open an
		hdfs file in O\_RDWR mode \\
		{\tt MPI\_File\_write\_shared} & {\tt WRONLY} & cannot open an
		hdfs file in O\_RDWR mode \\
		{\tt MPI\_File\_write\_shared} & {\tt APPEND} &file open. code:
		201388309\\\bottomrule 
	\end{tabular}
	\caption{\small Error codes for parallel writes on fuse-dfs.}
	\label{tab:write}
\end{table}
\bibliography{ref}{}
\bibliographystyle{acm}
\end{document}
