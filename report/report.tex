\documentclass[11pt,titlepage]{article}
% define the title
\author{Luna Xu (xuluna@cs.vt.edu) \and Adam Binford (adamq@vt.edu)}

\title{CS 5204 Project Final Report \\ A Feasible Study of MPI-IO on Top of HDFS}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
%\usepackage[pdftex,bookmarks,colorlinks]{hyperref}
\usepackage{cite}
\usepackage{color}
\usepackage{pgfgantt}
\usepackage{float}
\usepackage{url}
\usepackage{booktabs}

%fullpage â€“ Set all page margins to 1.5cm
\usepackage{fullpage}

\newcommand{\otoprule}{\midrule[\heavyrulewidth]}
%\usepackage[pdftex]{hyperref}
\begin{document}
% generates the title
\maketitle

\section{Team member}
Luna Xu (xuluna)\\
Adam Binford (adamq)

\section{Introduction}
MapReduce~\cite{mr} and its most popular implementation Hadoop~\cite{hadoop}
have become the dominant distributed processing framework for big data
analytics. Despite of the ease-of-use and scalibility of Hadoop, researchers
also found the limitations of Hadoop lie in for example, inter-process
communication. For such limitations, the well established Message Passing
Interface (MPI)~\cite{mpi} is more suitable due to its ability to support any
communication pattern. X.Lu et al~\cite{xlu} find that  the
message latency of MPI is about 100 times less than Hadoop
primitives. The average peak bandwidth of MPI is about
100 times higher than Hadoop RPC -- the fundamental communication
mechanism in Hadoop. Moreover, there exists data analytics workflows such as
Metagenomics~\cite{meta} that consist both compute- and communication-intensive
computations. To better conduct such workflows and to avoid data
movement between clusters~\cite{???}, resource coordination platforms such as
mesos~\cite{mesos}, omega~\cite{omega}, YARN~\cite{yarn} enable different
programming paradigms including MPI and MapReduce to co-exist in the same
cluster. Though not realized yet, hosting MPI and Hadoop in the same cluster is
highly promising as YARN claims to embrace MPI as a first class citizen.

One of the biggest challenges of co-hosting MPI and Hadoop is to decide the underlying shared file
system. As a big feature of MPI-2 standard~\cite{mpi}, MPI-IO provides parallel
IO support to MPI programs and enables MPI to process data-intensive workloads
as well. Currently, such support requires an underlying network/parallel file
system such as NFS~\cite{nfs1}, PVFS~\cite{pvfs}, Lustre~\cite{lustre},
GPFS~\cite{gpfs} to achieve the best performance. However, these file systems
are focused on optimization for MPI-IO~\cite{mpipvfs, mpilustre1, mpigpfs} and
have a big network overhead on hosting Hadoop with the absence of data
locality~\cite{hadooplustre}. IBM's GPFS is originally designed as a SAN file
system as the data is striped and placed in a round-robin fashion~\cite{gpfs},
which prevents it from being used in Hadoop. With a support of File Placement
Optimization (FPO), GPFS-FPO makes it possible to efficiently support Hadoop.
However, GPFS is shipped with IBM SP system and is not available as opensource
as Lustre. Moreover, IBM tailors MPI-IO according to GPFS in their own MPI
implementation~\cite{mpigpfs}, which is not supported in more wildly used
implementations such as MPICH~\cite{} and OpenMPI~\cite{openmpi}.

Another solution is to support MPI-IO on top of the distributed file systems
used by MapReduce such as GFS~\cite{} and HDFS~\cite{hdfs}. HDFS is integrated
inherently in Hadoop releases and is the default file system used in Hadoop
community. By bringing MPI to HDFS, it is possible to keep existing applications
in Hadoop ecosystem without any changes. C. Cranor et al~\cite{} explore the
performance of MPI-IO on HDFS using PLFS. However, HDFS is supported as a
component of PLFS and no data locality is achieved for MPI jobs. As far as we
know, there is no such work on supporting MPI on top of HDFS directly. This
project focuses on exploring the feasibility and performance of enabling MPI-IO
on top of HDFS. This study is based on the observation that MPI-IO provides
great flexibility that it is possible for users to decide the process-to-block
mapping based on the information that HDFS provides. 

{\color{blue}Challenges of enabling MPI-IO on top of HDFS.}

\section{Project Progress}
We have finished investigating the possible ways to mount HDFS as a regular
file system that can be interacted with by any file I/O. 
We got the throughput for manual copy, native NFS as the ideal performance,
fuse-dfs throughput for parallel read. However we could not perform parallel
write using fuse-dfs. Table~\ref{tab:write} shows the errors we encountered during our
tries. We tried using {\tt MPI\_File\_write\_at} where each process holds an
individual file pointer, as well as {\tt MPI\_File\_write\_shared} where all
processes hold a shared file pointer. We open the file using different mode and
with the combinations we get mainly two errors. The error we get from the {\tt
APPEND} mode is reported in the MPI program side, others are shown in the
fuse-dfs side. Another method that we explored is Native HDFS Fuse~\cite{native},
which utilizes only protobuf to communicate with Namenode directly. Hence no
fuse or native lib is involved. However, the program dumped a segmentation fault
when we tried to run. HDFS-NFS solution is also not successful nor
desirable because either it has requirements for specific (2.3.0) Hadoop
version~\cite{nfs} or it only supports the cloudera distribution of
Hadoop~\cite{proxy}.


We are now focusing on creating a library to 
hook MPI~\cite{mpich} I/O function calls to use the HDFS native library to interact with
HDFS. Our goal is to allow 
unmodified MPI applications to interact with HDFS by simply loading our library
at runtime. 
So far we have successfully hooked MPI functions at runtime, and verified our
functions were being 
called. Additionally, we have read from and written to files in our running HDFS
using the HDFS native 
library. When reading a single file from multiple processes, we have observed an
increase in bandwidth 
when increasing processes. This confirms that multiple processes can read from the
same file at once using 
the native library. To complement this work, we have developed scripts to
compile and run these HDFS 
native library applications easily.

\section{Future work}
The final steps we have to do are implementing the necessary MPI I/O functions
in our hooking library to 
use the HDFS native library as the file I/O method. We have already done each of
these pieces 
individually, hooking and using the native library, we simply must combine them.
The hooking functions 
need to be able to use the parameters they are given to seamlessly work with
HDFS without the MPI 
program knowing anything is different. 

Additionally, we must find out if it is possible to implement some support for
writing to HDFS through 
the hooked MPI routines. The native library only allows appending to a file, and
only one thread can 
access a file for writing at one time. We must either modify the behavior of the
I/O of the MPI program 
or set restrictions on what MPI programs running on HDFS are allowed to do.
Finally, we must simplify 
the scripts required for our solution to work to put as small of a burden on the
user as possible.

\begin{table}[t]
	\centering
	\small
	\begin{tabular}{ccc}
		\toprule
	{\bf Function} &{\bf Mode} &{\bf Error} \\\otoprule
		{\tt MPI\_File\_write\_at} & {\tt CREATE|RDWR} & cannot open an
		hdfs file in O\_RDWR mode \\
		{\tt MPI\_File\_write\_at} & {\tt CREATE|WRONLY} & cannot open an
		hdfs file in O\_RDWR mode \\
		{\tt MPI\_File\_write\_at} & {\tt WRONLY} & cannot open an
		hdfs file in O\_RDWR mode \\
		{\tt MPI\_File\_write\_shared} & {\tt WRONLY} & cannot open an
		hdfs file in O\_RDWR mode \\
		{\tt MPI\_File\_write\_shared} & {\tt APPEND} &file open. code:
		201388309\\\bottomrule 
	\end{tabular}
	\caption{\small Error codes for parallel writes on fuse-dfs.}
	\label{tab:write}
\end{table}
\bibliography{ref}{}
\bibliographystyle{acm}
\end{document}
